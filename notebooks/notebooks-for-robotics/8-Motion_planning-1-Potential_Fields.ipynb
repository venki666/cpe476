{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 Potential Fields - Moving in the mall\n",
    "\n",
    "After implementing the SLAM algorithm, the robots provided by **<span style=\"color:seagreen\">UMA-MR</span>** are able to simultaneously build maps of the malls and localize themselves within them. However, the **<span style=\"color:seagreen\">managers at Nirvana</span>** are looking for a fully operational robot, and something is still missing: the navigation between any two points in the malls. These points could be, for example, an information point, a shop entrance or a shop counter, a rescue point, a restaurant, etc.\n",
    "\n",
    "From previous developments, our team has an algorithm able to find a sequence of waypoints between the start point and the goal one, that is, to plannify a global navigation. So **our mission is here to develop an algorithm able to command the robot to safely navigate from a start waypoint to a (close) goal one, that is, to carry out local reactive navigation**. \n",
    "\n",
    "The image below shows an sketch of the restaurants area in the **<span style=\"color:seagreen\">Nirvana mall</span>**, along with an example of global navigation (blue waypoints and dotted lines) between the information point and the *Dino's* restaurant. In that example, the green dotted lines correspond with local reactive navigation avoiding obstacles in the path.\n",
    "\n",
    "<img src=\"images/8-Motion_planning/mall_navigation_example2.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalizing the problem\n",
    "\n",
    "The **reactive navigation** (or **local navigation**) has the objective of moving towards a close destination while avoiding obstacles. For that, it is available sensor data within a specific *look-ahed* as well as the goal point (**inputs**), being the reactive navigation method in charge of producing motor commands (**outputs**) to safely reach such goal.\n",
    "\n",
    "In this way, reactive navigation methods **does not require neither any kind of map of the environment nor memory of previous observations**. In practice, the last requirements usually arises since in some situations it could be useful to also consider the last sensor observations (e.g. while crossing a door).\n",
    "\n",
    "Finally, reactive navigation techniques **must run very fast** (i.e. real time or close to it) in order to safely reach the goal point. If not, dynamic obstacles or deprecated motion commands could lead the robot to crash!\n",
    "\n",
    "In summary: \n",
    "\n",
    "```\n",
    "reactive_navigation(current_location,target_location,sensor_readings)\n",
    "    # Method computations ... so fast!\n",
    "    return (v_l,v_r)\n",
    "```\n",
    "## Potential Fields\n",
    "\n",
    "**Potential Fields** is a popular and simple technique for carrying out reactive navigation. It consist of defining a **potential (energy) function** over the free space in the robot workspace, which has a **global minimum** at the goal and a maximum at obstacles. Then, in each iteration of the algorithm, the robot moves to a lower energy configuration, similar to a a ball rolling down a hill. To carry out such navigation the robot applies a force proportional to the **negated gradient of the potential field** (recall that the gradient always go in the direction in which the signal increases, and the robot pursues a lower energy, so it has to use the negated gradient).\n",
    "\n",
    "The **potential (energy) function** defines a **potential field** over the workspace. For each robot position in such workspace, the energy function is computed as:\n",
    "\n",
    "$$U(q)=U_{att}(q)+U_{rep}(q)$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $U_{att}(q)$ is the **atractive potential field**, which is retrieved by:\n",
    "\n",
    "  $$U_{att}(q)=\\frac{1}{2}K_{att}p^2_{goal}(q)$$\n",
    "\n",
    "  being $p_{goal}$ the distance from the robot to the goal: $p^2_{goal}=||q-q_{goal}||^2$ and $K_{att}$ a given gain, so this potential is higher for far distances, \n",
    "  <br />\n",
    "  \n",
    "- and $U_{rep}(q)$ is the **repulsive potential field**, computed as:\n",
    " \n",
    " $$U_{rep}(q)=  \\begin{cases} \n",
    "   \\frac{1}{2} K_{rep}(\\frac{1}{p(q)}-\\frac{1}{p_o})^2 & \\text{if } p(q) < p_o \\\\\n",
    "   0       & \\text{if } p(q) \\geq p_o\n",
    "  \\end{cases}$$\n",
    "  \n",
    "  being $p_o$ a given distance threshold, so obstacles far away from the robot does not influence the potential field, and $p(q)$ the distance of the object.\n",
    "\n",
    "Having defined such potential field, it can be computed a **force field** at the robot position $F(q)$ (a two-element vector) as the gradient of the previous one:\n",
    "\n",
    "$$\n",
    "F(q) = -\\nabla U(q) = -\\nabla U_{att}(q) - \\nabla U_{rep}(q) = \\begin{bmatrix} \\partial U / \\partial x \\\\ \\partial U / \\partial y \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $-\\nabla U_{att}(q)$ is also called the **attractive force** and \n",
    "- $-\\nabla U_{rep}(q)$ the **repulsive force**, so\n",
    "- $F(q)=F_{att}(q)+F_{rep}(q)$. \n",
    "\n",
    "Finally, the **robot speed (v_x,v_y)** is set proportional to the force $F(q)$ as generated by the field.\n",
    "\n",
    "The picture below illustrates all the elements in the computation of $F(q)$ ($F_{total}$ in the image):\n",
    "\n",
    "<img src=\"images/fig8-1-1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the Potential Fields method for Reactive navigation\n",
    "\n",
    "In order to obtain the sum of the forces that apply at a certain robot position, so they can be used to apply velocities to the robot wheels, you have to compute the attractive and repulsive forces (recall that $F(q)=F_{att}(q)+F_{rep}(q)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy import linalg\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.DrawRobot import DrawRobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Repulsive Force\n",
    "\n",
    "Let's start with the repulsive force (`FRep`) computation, which is the sum of the repulsive forces yielded by each obstacle close to the object. Recall that forces are 2-elements column vectors. \n",
    "\n",
    "The `repulsive_force()` function below partially implements this computation. It also plots a marker over the obstacles that have influence on this force, and store the handler of that plot in `hInfluentialObstacles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repulsive_force(xRobot, Map, RadiusOfInfluence, KObstacles):\n",
    "    \"\"\" Computes the respulsive force at a given robot position\n",
    "    \n",
    "        Args:\n",
    "            xRobot: Column vector containing the robot position ([x,y]')\n",
    "            Map: Matrix containing the obstacles coordinates (size 2xN_obstacles)\n",
    "            RadiusOfInfluence: distance threshold for considering that an obstacle has influence\n",
    "            KObstacles: gain related to the repulsive force\n",
    "        \n",
    "        Returns: Nothing. But it modifies the state in robot\n",
    "            Frep: repulsive force ([rf_x, rf_y]')\n",
    "            hInfluentialObstacles: handler of the plot marking the obstacles that have influence\n",
    "    \"\"\"        \n",
    "    q = xRobot - Map\n",
    "    r = np.sqrt(np.sum(q**2, axis=0))\n",
    "    iInfluential = np.where(r < RadiusOfInfluence)[0]\n",
    "    \n",
    "    if iInfluential.shape[0] > 0:\n",
    "        q = q[:, iInfluential]\n",
    "        r = r[iInfluential]\n",
    "        FRep = None\n",
    "        \n",
    "        hInfluentialObstacles = plt.plot(Map[0,iInfluential],Map[1,iInfluential],'kx')\n",
    "    else:\n",
    "        # Nothing close\n",
    "        FRep = None\n",
    "        hInfluentialObstacles = None # Do not modify this line! It's okey\n",
    "    \n",
    "    return FRep, hInfluentialObstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY IT!\n",
    "xRobot = np.vstack([[1],[2]])\n",
    "Map = np.vstack([[1.1, 2.4, 3.5],[2.2, 1.4, 4.5]])\n",
    "RadiusOfInfluence = 2\n",
    "KObstacles = 200\n",
    "\n",
    "FRep, handler = repulsive_force(xRobot, Map, RadiusOfInfluence, KObstacles)\n",
    "\n",
    "print ('Repulsive force:\\n ' + str(FRep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output:</span>\n",
    "\n",
    "```\n",
    "Repulsive force:\n",
    " [[ -7117.97589183]\n",
    " [-14205.83001107]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Attractive Force\n",
    "\n",
    "Next, you need to compute the Attractive Force `FAtt`. Normalize the resultant Force by $||\\Delta_{goal}||$. You can take a look at [linalg.norm()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html) for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attractive_force(KGoal, GoalError):\n",
    "    \"\"\" Computes the attractive force at a given robot position\n",
    "    \n",
    "        Args:\n",
    "            KGoal: gain related to the attractive force\n",
    "            GoalError: distance from the robot to the goal ([d_x d_y]')\n",
    "        \n",
    "        Returns: Nothing. But it modifies the state in robot\n",
    "            FAtt: attractive force ([af_x, af_y]')\n",
    "    \"\"\"           \n",
    "    FAtt = None\n",
    "    FAtt /= None # Normalization\n",
    "    \n",
    "    return FAtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY IT!\n",
    "KGoal = 1.5\n",
    "GoalError = np.vstack([[2.3],[1.4]]) \n",
    "\n",
    "FAtt = attractive_force(KGoal, GoalError)\n",
    "\n",
    "print ('Attractive force:\\n ' + str(FAtt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Expected output:</span>\n",
    "\n",
    "```\n",
    "Attractive force:\n",
    " [[-1.28129783]\n",
    " [-0.77992042]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Total Force\n",
    "\n",
    "Finally you can compute the Total Force `FTotal`. Do it in the main program below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(nObstacles=175,\n",
    "         MapSize=100,\n",
    "         RadiusOfInfluence=10,\n",
    "         KGoal=1,\n",
    "         KObstacles=250,\n",
    "         nMaxSteps=300,\n",
    "         NON_STOP=True):\n",
    "    \n",
    "    Map = MapSize*random.rand(2, nObstacles)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.ion()\n",
    "    ax.plot(Map[0,:],Map[1,:],'ro', fillstyle='none');\n",
    "    \n",
    "    fig.suptitle('Click to choose starting point:')\n",
    "    xStart = np.vstack(plt.ginput(1)).T\n",
    "    print('Starts at:\\n{}'.format(xStart))\n",
    "    \n",
    "    \n",
    "    fig.suptitle('Click to choose end goal:')\n",
    "    xGoal = np.vstack(plt.ginput(1)).T\n",
    "    print('Goal at:\\n{}'.format(xGoal))\n",
    "\n",
    "    fig.suptitle('')\n",
    "\n",
    "    ax.plot(xGoal[0, 0], xGoal[1, 0],'g*', markersize=10)\n",
    "    \n",
    "    hRob = DrawRobot(fig, ax, np.vstack([xStart, 0]), axis_percent=0.001, color='blue')\n",
    "    \n",
    "    # Initialization of useful vbles\n",
    "    xRobot = xStart\n",
    "    GoalError = xRobot - xGoal\n",
    "    \n",
    "    # Simulation\n",
    "    k = 0\n",
    "\n",
    "    while linalg.norm(GoalError) > 1 and k < nMaxSteps:\n",
    "\n",
    "        FRep, hInfluentialObstacles = repulsive_force(xRobot, Map, RadiusOfInfluence, KObstacles)\n",
    "        FAtt = attractive_force(KGoal, GoalError)\n",
    "                \n",
    "        # Point 1.3\n",
    "        # TODO Compute total (attractive+repulsive) potential field\n",
    "\n",
    "        FTotal = None\n",
    "        #FTotal /= linalg.norm(FTotal)\n",
    "        \n",
    "        xRobot += FTotal\n",
    "        Theta = np.arctan2(FTotal[1, 0], FTotal[0, 0])\n",
    "        \n",
    "        hRob.pop(0).remove()\n",
    "        hRob = DrawRobot(fig, ax, np.vstack([xRobot, Theta]), axis_percent=0.001, color='blue')\n",
    "        \n",
    "        if NON_STOP:\n",
    "            plt.pause(0.1)\n",
    "        else:\n",
    "            plt.waitforbuttonpress(-1)\n",
    "            \n",
    "        if hInfluentialObstacles is not None:\n",
    "            hInfluentialObstacles.pop(0).remove()\n",
    "        \n",
    "        # Update termination conditions\n",
    "        GoalError =  xRobot - xGoal\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding how the technique performs\n",
    "\n",
    "As a brilliant engineer, you have to provide some indications to the **<span style=\"color:seagreen\">managers at Nirvana</span>** about how the technique performs and its limitations. The following points help you to retrieve the needed information for that.  **<span style=\"color:orangered\">Your mission here is to provide such information in an appealing way in *your report to Nirvana*</span>**\n",
    "\n",
    "- 4.1 Discuss the meaning of each element appearing in the plot during the simulation of the *Potential Fields reactive navigation*.\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "  <img src=\"images/fig8-1-2.png\" width=\"400\" alt=\"\">\n",
    "  <figcaption>\n",
    "      Fig. 2: Execution of potential fields. <br/>\n",
    "      It shows the robots pose and trace (in blue), <br/>\n",
    "      obstacles (in red), influence (crossed-out obstacles), <br/>\n",
    "      and a goal (in green)\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4.2 Run the program setting different start and goal positions. Now change the values of the goal and obstacle gains (`KGoal` and `KObstacles`). How does this affect the paths followed by the robot?\n",
    "\n",
    "  Examples with different values for such constants:\n",
    "\n",
    "  <table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/fig8-1-3.png\" width=\"300\"></td>\n",
    "        <td><img src=\"images/fig8-1-4.png\" width=\"300\"></td>\n",
    "    </tr>\n",
    "  </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(KGoal=1, KObstacles=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4.3 Play with different numbers of obstacles and discuss the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(nObstacles=175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4.4 Illustrate a navigation where the robot doesn't reach the goal position in the specified number of steps. Why did that happen? Could the robot have reached the goal with more iterations of the algorithm? Hint: take a look at the ``FTotal`` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report to the managers\n",
    "\n",
    "In the cell below, include your report with the information that you consider interesting for the managers at **<span style=\"color:seagreen\">Nirvana</span>**. You should include pictures, text, and whatever you need to justify your developments. Don't forget to address the points posed in *4. Understanding how the technique performs*!:\n",
    "\n",
    " - (4.1) Discuss the meaning of each element appearing in the plot during the simulation of the Potential Fields reactive navigation.\n",
    " - (4.2) Run the program setting different start and goal positions. Now change the values of the goal and obstacle gains (``KGoal`` and ``KObstacles``). How does this affect the paths followed by the robot?\n",
    " - (4.3) Play with different numbers of obstacles and discuss the obtained results.\n",
    " - (4.4) Illustrate a navigation where the robot doesn't reach the goal position in the specified number of steps. Why did that happen? Could the robot have reached the goal with more iterations of the algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orangered\">***Your report here***</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
